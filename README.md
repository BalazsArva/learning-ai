# Project to learn and experiment with things related to AI

This simple application demonstrates the RAG pattern on a collection of simple mock knowledgebase documents.

The documents are sent to the API application's creation endpoint by the test data generator application. The creation process generates embedding vectors from the sample documents' contents and persists them along with the documents themselves.

The API's query endpoint supports arbitrary queries: terms-based and prompt-like, and it uses conversational LLM to detect which kind the user is doing. When a terms-based query (or when the type cannot be decided) is run, there is no AI response generation to the query (i.e. the user won't get an extract from the sources generated by LLM). When the input is promp-like, e.g. user inquirying about WFH policies, an LLM conversational model is fed with the documents that contain the required grounding information. These documents are located by generating embeddings from the query itself and using the database's vector similarity search capabilities to identify the ones that are most similar to the query's semantics.

A conversational LLM model is also used for generating keywords for searching for documents. These are used in all cases: prompt-like, terms-based and indefinite type queries. When the search is terms-based, the LLM is used to generate alternative terms (synonyms or phrases that are semantically similar to the original input) for greater likelyhood of finding matches. For prompt-like queries, we locate grounding documents by relying on both vector search as well as terms queries, the latter of which are searched for by asking the LLM to transform the prompt into a list of the most significant keywords as well as their synonyms or alternatives like in the previous case.

## Dependencies

* RavenDB - for storing knowledgebase documents and the vectors generated from them
* ollama - for generating embeddings
* Azure OpenAI - conversational LLM for generating responses to user prompts

## Running the application

The application should work as-is provided that the dependencies listed above are running and accessible at the configured location. There must also be a database created in RavenDB with the configured name (by default, `learning-ai`).

Docker can be used to run these dependencies, using the following commands:

* ollama

```bash
docker run --restart unless-stopped -d -p 11434:11434 --name ollama ollama/ollama
docker exec -it ollama ollama run all-minilm
```

* RavenDB - once up and running, you need to configure the server __[using the UI](http://localhost:8080)__. For local development purposes, just accept the defaults and choose the unsecured mode for ease of setup. After restart, a database can be created. When it prompts you for a license, you can __[request a free developer license](https://ravendb.net/license/request/dev)__.

```bash
docker run -d -p 8080:8080 --name ravendb --restart unless-stopped ravendb/ravendb:7.0-latest
```

* Azure OpenAI - create an Azure OpenAI resource in Azure and using the AI foundry portal, set up an LLM model. Currently, `gpt-4o` can be set up but the availability of this may change in the future. Configure the model, the endpoint and the API key for connecting to the service by using the API project's user secrets. It should have the following structure:

```json
{
  "AzureOpenAI": {
    "Endpoint": "<your-endpoint>",
    "ApiKey": "<your-api-key>",
    "ModelId": "gpt-4o"
  }
}
```

When everything is set up and ready, start the API then the data generator for inserting mock data. The API is accessible by using the [Scalara UI](http://localhost:5011/scalar/).
